{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "This project serves as a re-introduction to data science and machine learning for me. I want to go back to the fundamentals, understanding why each move is made and how it impacts the project overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"predictive_maintenance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Observations on the data\n",
    "- There is an identification number (UDI) that is unneccessary\n",
    "- The Product ID column is also unnecessary for any analysis carried out as it is a unique identifier.\n",
    "- This dataset has multiple target columns. Target and Failure Type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID columns\n",
    "\n",
    "df.drop(columns=['UDI','Product ID'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Type':'type', 'Air temperature [K]' :'air_temperature', 'Process temperature [K]':'process_temperature',\n",
    "       'Rotational speed [rpm]':'rotational_speed', 'Torque [Nm]': 'torque', 'Tool wear [min]': 'tool_wear', 'Target': 'target',\n",
    "       'Failure Type': 'failure_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tool_wear'] = df['tool_wear'].astype('float64')\n",
    "df['rotational_speed'] = df['rotational_speed'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each type\n",
    "type_counts = df['type'].value_counts()\n",
    "type_percentage = 100 * type_counts / df['type'].shape[0]\n",
    "\n",
    "# Prepare labels and values for the pie chart\n",
    "labels = type_percentage.index\n",
    "sizes = type_percentage.values\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 6))  # Optional: Set figure size for better readability\n",
    "plt.pie(sizes, labels=labels, colors=sns.color_palette('tab10')[:len(labels)], autopct='%.0f%%', startangle=90)\n",
    "plt.title('Machine Type Percentage')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie chart is drawn as a circle.\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good enough split that does not indicate too much of oversampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Anomalies\n",
    "\n",
    "Prior knowledge of this dataset confirms there are anomalies such as: \n",
    "- Values are classified as failure in the 'Target' variable but as No Failure in the 'failure_type' column.\n",
    "- Values are classifed as Random Failures by 'Failure Type', but they are classifed as No failure by the 'Target Variable'\n",
    "\n",
    "We have to eliminate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['failure_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failure = df[df['target'] == 1]\n",
    "df_failure['failure_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the first anomaly and those values will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify indices of rows with 'No Failure'\n",
    "position_wrong_failures = df_failure[df_failure['failure_type'] == 'No Failure'].index\n",
    "\n",
    "# Drop these indices from the original DataFrame\n",
    "df.drop(position_wrong_failures, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failure = df[df['target'] == 0]\n",
    "df_failure['failure_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the position of these random failures\n",
    "position_wrong_random_failures = df_failure[df_failure['failure_type'] == 'Random Failures'].index\n",
    "\n",
    "#drop the columns\n",
    "df.drop(position_wrong_random_failures, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 data points out of 10000 will be fine. 0.27%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max of `rotational_speed`, `torque` and `tool_wear` being significantly different from could indicate outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(2, 5, figsize=[25, 10])\n",
    "j = 0\n",
    "colors = ['#E1728F', '#409E7D']\n",
    "\n",
    "# List of features to plot\n",
    "features = ['air_temperature', 'process_temperature', 'rotational_speed', 'torque', 'tool_wear']\n",
    "\n",
    "for i in features:\n",
    "    # Histogram with KDE\n",
    "    sns.histplot(data=df, x=i, kde=True, ax=axes[0, j], hue='target', palette=colors)\n",
    "    \n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df, x=i, ax=axes[1, j], palette=['#976EBD'])\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are obviously outliers in the data to be dealt with later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "There were class balances as we could see which is a huge problem in machine learning problems. Some ways of solving class imbalances involve: \n",
    "\n",
    "- Under-sampling by deleting some data points from the majority class.\n",
    "- Over-Sampling by copying rows of data resulting in the minority class.\n",
    "- Over-Sampling with SMOTE (Synthetic Minority Oversampling Technique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fail = df[df['failure_type'] != 'No Failure']\n",
    "\n",
    "# Calculate failure type percentages\n",
    "failure_counts = df_fail['failure_type'].value_counts()\n",
    "df_fail_percentage = 100 * failure_counts / failure_counts.sum()\n",
    "\n",
    "# Calculate overall failure percentage in the data\n",
    "total_failures = df['target'].sum()\n",
    "total_records = len(df)\n",
    "overall_failure_percentage = round(100 * total_failures / total_records, 2)\n",
    "\n",
    "# Print overall failure percentage\n",
    "print('Failures percentage in data:', overall_failure_percentage)\n",
    "print('Percentage of no failure in data:', 100 - overall_failure_percentage)\n",
    "\n",
    "# Create a pie plot for failure causes\n",
    "plt.title('Reasons for Machine Failures')\n",
    "plt.pie(\n",
    "    x=df_fail_percentage,\n",
    "    labels=df_fail_percentage.index,\n",
    "    colors=sns.color_palette('tab10')[0:4],\n",
    "    autopct='%.0f%%'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96% of the data is a huge imbalance and we work to correct using SMOTE analysis. A link to an article written on the reasoning behind using that form of data augmentation will be included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "# Determine the number of 'No Failure' instances, which should represent 80% of the desired length\n",
    "number_no_failure = df['failure_type'].value_counts()['No Failure']\n",
    "desired_length = round(number_no_failure / 0.8)\n",
    "\n",
    "# Calculate the number of samples needed for each failure type\n",
    "samples_per_class = round((desired_length - number_no_failure) / 4)  # Distributing among four failure types\n",
    "\n",
    "# Define the resampling strategy\n",
    "resampling_strategy = {\n",
    "    'No Failure': number_no_failure,\n",
    "    'Overstrain Failure': samples_per_class,\n",
    "    'Heat Dissipation Failure': samples_per_class,\n",
    "    'Power Failure': samples_per_class,\n",
    "    'Tool Wear Failure': samples_per_class\n",
    "}\n",
    "\n",
    "# Initialize the SMOTENC instance for categorical features\n",
    "smote = SMOTENC(categorical_features=[0, 7], sampling_strategy=resampling_strategy, random_state=0)\n",
    "\n",
    "# Resample the DataFrame\n",
    "df_resampled, y_resampled = smote.fit_resample(df, df['failure_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Identify failures in the resampled DataFrame\n",
    "failure_indices_resampled = df_resampled[df_resampled['failure_type'] != 'No Failure'].index\n",
    "df_res_fail = df_resampled.loc[failure_indices_resampled]\n",
    "\n",
    "# Calculate failure type percentages in the resampled DataFrame\n",
    "failure_counts_resampled = df_res_fail['failure_type'].value_counts()\n",
    "fail_res_percentage = 100 * failure_counts_resampled / df_res_fail.shape[0]\n",
    "\n",
    "# Calculate percentage increment of observations after oversampling\n",
    "percentage_increment = round((df_resampled.shape[0] - df.shape[0]) * 100 / df.shape[0], 2)\n",
    "\n",
    "# Calculate percentage of failures in the resampled DataFrame\n",
    "smote_resampled_failures_percentage = round(df_res_fail.shape[0] * 100 / df_resampled.shape[0], 2)\n",
    "\n",
    "# Print results\n",
    "print('Percentage increment of observations after oversampling:', percentage_increment)\n",
    "print('SMOTE Resampled Failures percentage:', smote_resampled_failures_percentage)\n",
    "\n",
    "# Create pie plots for failure causes\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "fig.suptitle('Causes Involved in Machine Failures')\n",
    "\n",
    "# Original failures percentage\n",
    "axs[0].pie(\n",
    "    x=df_fail_percentage,\n",
    "    labels=df_fail_percentage.index,\n",
    "    colors=sns.color_palette('tab10')[0:4],\n",
    "    autopct='%.0f%%'\n",
    ")\n",
    "axs[0].title.set_text('Original')\n",
    "\n",
    "# Resampled failures percentage\n",
    "axs[1].pie(\n",
    "    x=fail_res_percentage,\n",
    "    labels=fail_res_percentage.index,\n",
    "    colors=sns.color_palette('tab10')[0:4],\n",
    "    autopct='%.0f%%'\n",
    ")\n",
    "axs[1].title.set_text('After Resampling')\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Encoding\n",
    "\n",
    "Scaling data is a difficult process as you have to choose the proper scaler to use \n",
    "- Use MinMaxScaler as your default\n",
    "- Use RobustScaler if you have outliers and can handle a larger range\n",
    "- Use StandardScaler if you need normalized features\n",
    "- Use Normalizer sparingly - it normalizes rows, not columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [feature for feature in features if df[feature].dtype=='float64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define dictionaries for encoding categorical variables\n",
    "type_encoding = {'L': 0, 'M': 1, 'H': 2}\n",
    "cause_encoding = {\n",
    "    'No Failure': 0,\n",
    "    'Power Failure': 1,\n",
    "    'Overstrain Failure': 2,\n",
    "    'Heat Dissipation Failure': 3,\n",
    "    'Tool Wear Failure': 4\n",
    "}\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "df_preprocessed = df_resampled.copy()\n",
    "\n",
    "# Encoding categorical variables without using inplace\n",
    "df_preprocessed['type'] = df_preprocessed['type'].replace(to_replace=type_encoding)\n",
    "df_preprocessed['failure_type'] = df_preprocessed['failure_type'].replace(to_replace=cause_encoding)\n",
    "\n",
    "# Scaling numeric features\n",
    "df_preprocessed[num_features] = scaler.fit_transform(df_preprocessed[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA instance with the number of components equal to the number of features\n",
    "pca = PCA(n_components=len(num_features))\n",
    "\n",
    "# Fit PCA and transform the data, creating a DataFrame for the principal components\n",
    "X_pca = pd.DataFrame(\n",
    "    data=pca.fit_transform(df_preprocessed[num_features]),\n",
    "    columns=[f'PC{i+1}' for i in range(len(num_features))]\n",
    ")\n",
    "\n",
    "# Calculate the explained variance ratio as a percentage\n",
    "var_exp = pd.Series(\n",
    "    data=100 * pca.explained_variance_ratio_,\n",
    "    index=[f'PC{i+1}' for i in range(len(num_features))]\n",
    ")\n",
    "\n",
    "# Print the explained variance ratio per component\n",
    "print('Explained variance ratio per component:')\n",
    "print(round(var_exp, 2), sep='\\n')\n",
    "\n",
    "# Print the sum of the explained variance ratio for the first three components\n",
    "explained_variance_three_components = round(var_exp.values[:3].sum(), 2)\n",
    "print(f'Explained variance ratio with 3 components: {explained_variance_three_components}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for Data visualization\n",
    "pca3 = PCA(n_components=3)\n",
    "X_pca3 = pd.DataFrame(\n",
    "    data=pca3.fit_transform(df_preprocessed[num_features]),\n",
    "      columns=['PC1','PC2','PC3'])\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(18,4))\n",
    "fig.suptitle('Loadings magnitude')\n",
    "\n",
    "pca_loadings = pd.DataFrame(data=pca3.components_, columns=num_features)\n",
    "for j in range(3):\n",
    "    ax = axs[j]\n",
    "    sns.barplot(ax=ax, x=pca_loadings.columns, y=pca_loadings.values[j])\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.title.set_text('PC'+str(j+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "mask = np.triu(np.ones_like(df_preprocessed.corr(), dtype=np.bool))\n",
    "heatmap = sns.heatmap(df_preprocessed.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling \n",
    "\n",
    "This dataset allows us to perform the two different types of classification. Binary and Multi-class classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "import time\n",
    "\n",
    "# train-validation-test split\n",
    "X, y = df_preprocessed[features], df_preprocessed[['target','failure_type']]\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.1, stratify=df_preprocessed['failure_type'], random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.11, stratify=y_trainval['failure_type'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
